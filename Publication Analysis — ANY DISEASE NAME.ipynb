{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication Analysis â€“ Any Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put your search term here and run the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANY_DISEASE_NAME = 'Put your search term here and run the notebook!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, lxml, sys, sent2vec, nltk, cProfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import bayesian_changepoint_detection.offline_changepoint_detection as offcd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import partial\n",
    "from wordcloud import WordCloud\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "model_path = 'biosentvec.bin'\n",
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = ANY_DISEASE_NAME\n",
    "\n",
    "BASE_URL = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\n",
    "MIN_PAPERS = 50 # minimum number of papers in a year to use for analysis\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesses text before BioSentVec embedding\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# computes Euclidean distance\n",
    "def dist(x, y):\n",
    "    return np.linalg.norm(x - y)\n",
    "\n",
    "# converts a list to a comma separated values format\n",
    "def list_to_csv(lst):\n",
    "    ret = ''\n",
    "    for i, val in enumerate(lst):\n",
    "        ret += f'{val},'\n",
    "    return ret[:-1]\n",
    "\n",
    "# returns the UIDs of papers based on searching the title for a query\n",
    "def get_uids(query):\n",
    "    uid_list = []\n",
    "\n",
    "    retstart = 0\n",
    "    RETMAX = 100000\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': f'{query}[title]',\n",
    "            'retmax': RETMAX,\n",
    "            'retstart': retstart,\n",
    "        }\n",
    "        res = requests.get(f'{BASE_URL}esearch.fcgi', params)\n",
    "        soup = BeautifulSoup(res.text, 'xml')\n",
    "        uids = soup.find_all('Id')\n",
    "        num_uids = len(uids)\n",
    "        if num_uids == 0:\n",
    "            print(f'Found {len(uid_list)} UIDs corresponding to {query}.')\n",
    "            return uid_list\n",
    "        else:\n",
    "            uid_list = uid_list + [uid.text for uid in uids]\n",
    "            retstart += RETMAX\n",
    "            \n",
    "# returns the titles, abstracts, years, and year_span of publications corresponding to input UIDs          \n",
    "def get_summaries(uids):\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    years = []\n",
    "\n",
    "    retstart = 0\n",
    "    MAX_REQUEST = 10000 # number of uids to request at a time\n",
    "\n",
    "    sys.stdout.write(f'\\rRetrieved {retstart} / {len(uids)} titles.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    while (retstart < len(uids)):\n",
    "        query_uids = uids[retstart:retstart + MAX_REQUEST]\n",
    "        params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': list_to_csv(query_uids),\n",
    "        'retmax': MAX_REQUEST,\n",
    "        'retmode': 'xml',\n",
    "        'rettyle': 'abstract,'\n",
    "        }\n",
    "        res = requests.post(f'{BASE_URL}efetch.fcgi', params)\n",
    "        soup = BeautifulSoup(res.text, 'xml')\n",
    "\n",
    "        soup_articles = soup.find_all('PubmedArticle')\n",
    "\n",
    "        for soup_article in soup_articles:\n",
    "            soup_title = soup_article.find('ArticleTitle')\n",
    "            soup_date = soup_article.find('PubDate')\n",
    "            soup_abstract = soup_article.find('AbstractText')\n",
    "\n",
    "            if soup_title is None or soup_date is None or soup_abstract is None or soup_date.find('Year') is None:\n",
    "                continue\n",
    "\n",
    "            titles.append(soup_title.text.replace('\\xa0', ' ').replace('<i>', '').replace('</i>', ''))\n",
    "            years.append(int(soup_date.find('Year').text))\n",
    "            abstracts.append(soup_abstract.text)\n",
    "\n",
    "        retstart += MAX_REQUEST\n",
    "\n",
    "        sys.stdout.write(f'\\rRetrieved {min(retstart, len(uids))} / {len(uids)} summaries.')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    titles, abstracts, years = np.array(titles), np.array(abstracts), np.array(years)\n",
    "    \n",
    "    titles = titles[~np.isnan(years)]\n",
    "    abstracts = abstracts[~np.isnan(years)]\n",
    "    years = years[~np.isnan(years)]\n",
    "    \n",
    "    titles_mod, abstracts_mod, years_mod = [], [], []\n",
    "    \n",
    "    for title, abstract, year in zip(titles, abstracts, years):\n",
    "        if len(years[years == year]) >= MIN_PAPERS:\n",
    "            titles_mod.append(title)\n",
    "            abstracts_mod.append(abstract)\n",
    "            years_mod.append(year)\n",
    "            \n",
    "    titles, abstracts, years = np.array(titles_mod), np.array(abstracts_mod), np.array(years_mod)\n",
    "    \n",
    "    year_span = np.unique(years)\n",
    "\n",
    "    print(f'\\nAfter filtering, retrieved {len(titles)} summaries.')\n",
    "        \n",
    "    return titles, abstracts, years, year_span\n",
    "\n",
    "def get_titles_and_abstracts_and_years(term):\n",
    "    return get_summaries(get_uids(term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load publication data from NCBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, abstracts, years, year_span = get_titles_and_abstracts_and_years(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of publications vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_papers = [np.sum(years == year) for year in year_span]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.title(f'{term} Papers Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.plot(year_span, num_papers, color='b')\n",
    "plt.scatter(year_span, num_papers, color='b', s=100)\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title length vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array([len(title) for title in titles])\n",
    "average_lengths = [np.mean(lengths[years == year]) for year in year_span]\n",
    "stdev_lengths = [np.std(lengths[years == year]) for year in year_span]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(year_span, average_lengths, color='b', s=100)\n",
    "plt.plot(year_span, average_lengths, color='b')\n",
    "plt.errorbar(year_span, average_lengths, yerr=stdev_lengths, fmt='none', color='b', capsize=20)\n",
    "plt.title(f'Yearly Title Length for {term}')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Title Length (characters)')\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title entropies vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "v = vectorizer.fit_transform(titles).toarray()\n",
    "frequencies_unsorted = np.sum(v, axis=0)\n",
    "features_unsorted = vectorizer.get_feature_names()\n",
    "features, frequencies = zip(*sorted(zip(features_unsorted, frequencies_unsorted), key=lambda f: -f[1]))\n",
    "\n",
    "probabilities = []\n",
    "\n",
    "for year in year_span:\n",
    "    t = v[years == year]\n",
    "    probabilities.append(np.sum(t, axis=0) / np.sum(np.sum(t, axis=0)))\n",
    "\n",
    "entropies = [-np.sum(np.multiply(probability, np.nan_to_num(np.log2(probability), neginf=0))) for probability in probabilities]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(year_span, entropies, color='b')\n",
    "plt.scatter(year_span, entropies, color='b', s=100)\n",
    "plt.title(f'Yearly Entropy of Titles for {term}')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Entropy of Titles')\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words in publication titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plot = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_per_title = np.divide(frequencies, len(years))\n",
    "\n",
    "frequencies_plot, features_plot = [], []\n",
    "for frequency, feature in zip(frequencies_per_title, features):\n",
    "    if feature not in stop_words:\n",
    "        frequencies_plot.append(frequency)\n",
    "        features_plot.append(feature)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.title(f'Word Frequencies in {term} Titles')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency per Title')\n",
    "plt.bar(range(num_plot), frequencies_plot[1:num_plot+1], color='b')\n",
    "plt.xticks(range(num_plot), features_plot[1:num_plot+1])\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=-90, size=30)\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency changepoint analysis in publication titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider words whose average title frequency is at least FREQUENCY_THRESHOLD\n",
    "FREQUENCY_THRESHOLD = 0.05\n",
    "\n",
    "# only consider words with a changepoint probability of at least PROBABILITY_THRESHOLD\n",
    "PROBABILITY_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_distributions = []\n",
    "for year in year_span:\n",
    "    t = v[years == year]\n",
    "    word_distributions.append(np.sum(t, axis=0) / np.sum(years == year))\n",
    "word_distributions = np.transpose(word_distributions)\n",
    "\n",
    "for feature in features:\n",
    "    if np.mean(word_distributions[features_unsorted.index(feature)]) < FREQUENCY_THRESHOLD:\n",
    "        break\n",
    "    \n",
    "    data_unnormalized = word_distributions[features_unsorted.index(feature)]\n",
    "    \n",
    "    data = []\n",
    "    for i, year in enumerate(year_span):\n",
    "        if data_unnormalized[i] == 0:\n",
    "            data.append(0)\n",
    "        else:\n",
    "            data.append(data_unnormalized[i] / np.max(data_unnormalized) * 100)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    Q, P, Pcp = offcd.offline_changepoint_detection(data, partial(offcd.const_prior, l=(len(data)+1)), offcd.gaussian_obs_log_likelihood, truncate=-40)\n",
    "    probs = np.exp(Pcp).sum(0)\n",
    "    \n",
    "    if np.max(probs) < PROBABILITY_THRESHOLD:\n",
    "        continue\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(year_span, word_distributions[features_unsorted.index(feature)], color='b')\n",
    "    plt.scatter(year_span, word_distributions[features_unsorted.index(feature)], color='b', s=100)\n",
    "    plt.title(f'Word Frequency Per Title for {term}: {feature}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Frequency Per Title')\n",
    "    plt.rc('font', size=20)\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.title(f'Changepoint Analysis for {term}: {feature}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Changepoint Probability')\n",
    "    plt.plot(year_span[1:], probs, color='r')\n",
    "    plt.scatter(year_span[1:], probs, color='r', s=100)\n",
    "    plt.rc('font', size=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate title vectors with BioSentVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_processed = [preprocess_sentence(title) for title in titles]\n",
    "title_vectors = model.embed_sentences(titles_processed)\n",
    "titles_reduced = PCA(n_components=2).fit_transform(title_vectors)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(titles_reduced.T[0], titles_reduced.T[1], c=years, cmap='plasma')\n",
    "plt.title(f'{term} Title Embeddings')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.rc('font', size=20)\n",
    "plt.colorbar().set_label('Year', rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_span = np.array([np.mean(title_vectors[years == year], axis=0) for year in year_span])\n",
    "titles_span_reduced = PCA(n_components=2).fit_transform(titles_span)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(titles_span_reduced.T[0], titles_span_reduced.T[1], c=year_span, cmap='plasma', s=100)\n",
    "plt.title(f'{term} Average Title Embeddings by Year')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar().set_label('Year', rotation=90)\n",
    "\n",
    "plot_1 = titles_span_reduced[:-1]\n",
    "plot_2 = titles_span_reduced[1:]\n",
    "for i, p1 in enumerate(plot_1):\n",
    "    p2 = plot_2[i]\n",
    "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], color='gray')\n",
    "\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify research trends by identifying the paper titles with embeddings closest to the mean for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of representative papers to list closest to the mean\n",
    "NUMBER_OF_PAPERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for title_mean, year in zip(titles_span, year_span):\n",
    "    titles_mod = titles[year == years]\n",
    "    title_vectors_mod = title_vectors[year == years]\n",
    "    ds = [dist(title_mean, t_v) for t_v in title_vectors_mod]\n",
    "    ds_sorted, titles_sorted = zip(*sorted(zip(ds, titles_mod), key=lambda x: x[0]))\n",
    "    print(f'Year: {year}')\n",
    "    print()\n",
    "    for i in range(NUMBER_OF_PAPERS):\n",
    "        try:\n",
    "            print(f'Distance to mean: {ds_sorted[i]}')\n",
    "            print(f'Title: {titles_sorted[i]}')\n",
    "            print()\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify novelty of each year by the proportion of papers whose title embeddings are far from previous years' title embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold distance to consider a paper to be novel\n",
    "CUTOFF_DISTANCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = year_span[0]\n",
    "\n",
    "titles_prev = titles[years == year]\n",
    "title_vectors_prev = title_vectors[years == year]\n",
    "years_prev = years[years == year]\n",
    "\n",
    "counts = []\n",
    "\n",
    "for year in year_span[1:]:\n",
    "    titles_curr = titles[years == year]\n",
    "    title_vectors_curr = title_vectors[years == year]\n",
    "    years_curr = years[years == year]\n",
    "    \n",
    "    ds = []\n",
    "    titles_r, years_r = [], []\n",
    "    \n",
    "    count = 0.0\n",
    "    \n",
    "    for (title_curr, title_vector_curr, year_curr) in zip(titles_curr, title_vectors_curr, years_curr):\n",
    "        min_distance = np.inf\n",
    "        title_min, title_vector_min, year_min = None, None, None\n",
    "        title_rmin, title_vector_rmin, year_rmin = None, None, None\n",
    "        for (title_prev, title_vector_prev, year_prev) in zip(titles_prev, title_vectors_prev, years_prev):\n",
    "            if dist(title_vector_curr, title_vector_prev) < min_distance:\n",
    "                min_distance = dist(title_vector_curr, title_vector_prev)\n",
    "                title_min, title_vector_min, year_min = title_curr, title_vector_curr, year_curr\n",
    "                title_rmin, title_vector_rmin, year_rmin = title_prev, title_vector_prev, year_prev\n",
    "                \n",
    "        ds.append(min_distance)\n",
    "        titles_r.append(title_rmin)\n",
    "        years_r.append(year_rmin)\n",
    "        \n",
    "        if min_distance > CUTOFF_DISTANCE:\n",
    "            count += 1\n",
    "                \n",
    "    ds_sorted, titles_r_sorted, years_r_sorted, titles_curr_sorted = zip(*sorted(zip(ds, titles_r, years_r, titles_curr), key=lambda x: -x[0]))\n",
    "                                    \n",
    "    titles_prev = np.concatenate((titles_prev, titles_curr))\n",
    "    title_vectors_prev = np.concatenate((title_vectors_prev, title_vectors_curr))\n",
    "    years_prev = np.concatenate((years_prev, years_curr))\n",
    "    \n",
    "    counts.append(count / len(years_curr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(year_span[1:], counts, color='b', s=100)\n",
    "plt.plot(year_span[1:], counts, color='b')\n",
    "plt.title(f'Semantic Distance Novelty for {term} from Titles')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of Novel Papers')\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify clusters of novel research based on paper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify research papers published between year CHANGEPOINT and CHANGEPOINT + DELTA,  such that\n",
    "# (the mean distance between the paper title and its K nearest neighbors published before CHANGEPOINT)\n",
    "# is at least RATIO_THRESHOLD times (the mean distance between the paper title and its K nearest neighbors\n",
    "# also published between CHANGEPOINT and CHANGEPOINT + DELTA)\n",
    "CHANGEPOINT = 2000\n",
    "K = 3\n",
    "DELTA = 5\n",
    "RATIO_THRESHOLD = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_old, title_vectors_old, years_old = [], [], []\n",
    "titles_new, title_vectors_new, years_new = [], [], []\n",
    "\n",
    "for title, title_vector, year in zip(titles, title_vectors, years):\n",
    "    if year < CHANGEPOINT:\n",
    "        titles_old.append(title)\n",
    "        title_vectors_old.append(title_vector)\n",
    "        years_old.append(year)\n",
    "    if year >= CHANGEPOINT and year <= CHANGEPOINT + DELTA:\n",
    "        titles_new.append(title)\n",
    "        title_vectors_new.append(title_vector)\n",
    "        years_new.append(year)\n",
    "            \n",
    "for title, title_vector, year in zip(titles_new, title_vectors_new, years_new):\n",
    "    new_distance = np.mean(sorted([dist(title_vector, t_v) for t_v in title_vectors_new])[1:][:K])\n",
    "    old_distance = np.mean(sorted([dist(title_vector, t_v) for t_v in title_vectors_old])[:K])\n",
    "    \n",
    "    if (old_distance / new_distance) > RATIO_THRESHOLD:\n",
    "        print(f'Year: {year}')\n",
    "        print(f'Ratio: {old_distance / new_distance}')\n",
    "        print(f'New Distance: {new_distance}')\n",
    "        print(f'Old Distance: {old_distance}')\n",
    "        print(f'Title: {title}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create differential wordclouds for paper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a differential wordcloud for words appearing in year2, vs all words appearing before or during year1\n",
    "def create_wordcloud(year1, year2):\n",
    "    freqs_prev = np.sum(v[years <= year1], axis=0) / np.sum(years <= year1)\n",
    "    freqs_new = np.sum(v[years == year2], axis=0) / np.sum(years == year2)\n",
    "\n",
    "    diffs = np.subtract(freqs_prev, freqs_new)\n",
    "\n",
    "    pos_features, neg_features = set(), set()\n",
    "\n",
    "    d = {}\n",
    "    for diff, feature in zip(diffs, features_unsorted):\n",
    "        if feature in stop_words:\n",
    "            continue\n",
    "\n",
    "        diff = round(diff * 1000)\n",
    "        if diff > 0:\n",
    "            d[feature] = diff\n",
    "            pos_features.add(feature)\n",
    "        if diff < 0:\n",
    "            neg_features.add(feature) \n",
    "            d[feature] = -diff\n",
    "\n",
    "    def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        if word in pos_features:\n",
    "            return \"rgb(0, 0, 0)\"\n",
    "        if word in neg_features:\n",
    "            return \"rgb(256, 0, 0)\"\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white', height=500, width=1000)\n",
    "    wordcloud.generate_from_frequencies(frequencies=d)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud.recolor(color_func=color_func, random_state=3),\n",
    "               interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_wordcloud(1999, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract length vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array([len(abstract) for abstract in abstracts])\n",
    "average_lengths = [np.mean(lengths[years == year]) for year in year_span]\n",
    "stdev_lengths = [np.std(lengths[years == year]) for year in year_span]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(year_span, average_lengths, color='b', s=100)\n",
    "plt.plot(year_span, average_lengths, color='b')\n",
    "plt.errorbar(year_span, average_lengths, yerr=stdev_lengths, fmt='none', color='b', capsize=20)\n",
    "plt.title(f'Weekly Abstract Length for {term}')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Abstract Length (characters)')\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract entropies vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "v = vectorizer.fit_transform(abstracts).toarray()\n",
    "frequencies_unsorted = np.sum(v, axis=0)\n",
    "features_unsorted = vectorizer.get_feature_names()\n",
    "features, frequencies = zip(*sorted(zip(features_unsorted, frequencies_unsorted), key=lambda f: -f[1]))\n",
    "\n",
    "probabilities = []\n",
    "\n",
    "for year in year_span:\n",
    "    t = v[years == year]\n",
    "    probabilities.append(np.sum(t, axis=0) / np.sum(np.sum(t, axis=0)))\n",
    "\n",
    "entropies = [-np.sum(np.multiply(probability, np.nan_to_num(np.log2(probability), neginf=0))) for probability in probabilities]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(year_span, entropies, color='b')\n",
    "plt.scatter(year_span, entropies, color='b', s=100)\n",
    "plt.title(f'Yearly Entropy of Abstracts for {term}')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Entropy of Abstracts')\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words in abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plot = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_per_abstract = np.divide(frequencies, len(years))\n",
    "\n",
    "frequencies_plot, features_plot = [], []\n",
    "for frequency, feature in zip(frequencies_per_abstract, features):\n",
    "    if feature not in stop_words:\n",
    "        frequencies_plot.append(frequency)\n",
    "        features_plot.append(feature)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.title(f'Word Frequencies in {term} Abstracts')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency per Abstract')\n",
    "plt.bar(range(num_plot), frequencies_plot[1:num_plot+1], color='b')\n",
    "plt.xticks(range(num_plot), features_plot[1:num_plot+1])\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=-90, size=30)\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency changepoint analysis in publication abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider words whose average abstract frequency is at least FREQUENCY_THRESHOLD\n",
    "FREQUENCY_THRESHOLD = 0.05\n",
    "\n",
    "# only consider words with a changepoint probability of at least PROBABILITY_THRESHOLD\n",
    "PROBABILITY_THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_distributions = []\n",
    "for year in year_span:\n",
    "    t = v[years == year]\n",
    "    word_distributions.append(np.sum(t, axis=0) / np.sum(years == year))\n",
    "word_distributions = np.transpose(word_distributions)\n",
    "\n",
    "for feature in features:\n",
    "    if np.mean(word_distributions[features_unsorted.index(feature)]) < FREQUENCY_THRESHOLD:\n",
    "        break\n",
    "    \n",
    "    data_unnormalized = word_distributions[features_unsorted.index(feature)]\n",
    "    \n",
    "    data = []\n",
    "    for i, year in enumerate(year_span):\n",
    "        if data_unnormalized[i] == 0:\n",
    "            data.append(0)\n",
    "        else:\n",
    "            data.append(data_unnormalized[i] / np.max(data_unnormalized) * 100)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    Q, P, Pcp = offcd.offline_changepoint_detection(data, partial(offcd.const_prior, l=(len(data)+1)), offcd.gaussian_obs_log_likelihood, truncate=-40)\n",
    "    probs = np.exp(Pcp).sum(0)\n",
    "    \n",
    "    if np.max(probs) < PROBABILITY_THRESHOLD:\n",
    "        continue\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(year_span, word_distributions[features_unsorted.index(feature)], color='b')\n",
    "    plt.scatter(year_span, word_distributions[features_unsorted.index(feature)], color='b', s=100)\n",
    "    plt.title(f'Word Frequency Per Abstract for {term}: {feature}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Frequency Per Abstract')\n",
    "    plt.rc('font', size=20)\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.title(f'Changepoint Analysis for {term}: {feature}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Changepoint Probability')\n",
    "    plt.plot(year_span[1:], probs, color='r')\n",
    "    plt.scatter(year_span[1:], probs, color='r', s=100)\n",
    "    plt.rc('font', size=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate abstract vectors with BioSentVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_processed = [preprocess_sentence(abstract) for abstract in abstracts]\n",
    "abstract_vectors = model.embed_sentences(abstracts_processed)\n",
    "abstracts_reduced = PCA(n_components=2).fit_transform(abstract_vectors)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(abstracts_reduced.T[0], abstracts_reduced.T[1], c=years, cmap='plasma')\n",
    "plt.title(f'{term} Abstract Embeddings')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.rc('font', size=20)\n",
    "plt.colorbar().set_label('Year', rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_span = np.array([np.mean(abstract_vectors[years == year], axis=0) for year in year_span])\n",
    "abstracts_span_reduced = PCA(n_components=2).fit_transform(abstracts_span)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(abstracts_span_reduced.T[0], abstracts_span_reduced.T[1], c=year_span, cmap='plasma', s=100)\n",
    "plt.title(f'{term} Average Abstract Embeddings by Year')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar().set_label('Year', rotation=90)\n",
    "\n",
    "plot_1 = abstracts_span_reduced[:-1]\n",
    "plot_2 = abstracts_span_reduced[1:]\n",
    "for i, p1 in enumerate(plot_1):\n",
    "    p2 = plot_2[i]\n",
    "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], color='gray')\n",
    "\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify research trends by identifying the paper abstracts with embeddings closest to the mean for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of representative papers to list closest to the mean\n",
    "NUMBER_OF_PAPERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for abstract_mean, year in zip(abstracts_span, year_span):\n",
    "    abstracts_mod = abstracts[year == years]\n",
    "    titles_mod = titles[year == years]\n",
    "    abstract_vectors_mod = abstract_vectors[year == years]\n",
    "    ds = [dist(abstract_mean, t_v) for t_v in abstract_vectors_mod]\n",
    "    ds_sorted, abstracts_sorted, titles_sorted = zip(*sorted(zip(ds, abstracts_mod, titles_mod), key=lambda x: x[0]))\n",
    "    print(f'Year: {year}')\n",
    "    print()\n",
    "    for i in range(NUMBER_OF_PAPERS):\n",
    "        try:\n",
    "            print(f'Distance to mean: {ds_sorted[i]}')\n",
    "            print(f'Title: {titles_sorted[i]}')\n",
    "            print()\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify novelty of each year by the proportion of papers whose abstract embeddings are far from previous years' abstract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold distance to consider a paper to be novel\n",
    "CUTOFF_DISTANCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = year_span[0]\n",
    "\n",
    "abstracts_prev = abstracts[years == year]\n",
    "abstract_vectors_prev = abstract_vectors[years == year]\n",
    "abstracts_prev = years[years == year]\n",
    "\n",
    "counts = []\n",
    "\n",
    "for year in year_span[1:]:\n",
    "    abstracts_curr = abstracts[years == year]\n",
    "    abstract_vectors_curr = abstract_vectors[years == year]\n",
    "    years_curr = years[years == year]\n",
    "    \n",
    "    ds = []\n",
    "    abstracts_r, years_r = [], []\n",
    "    \n",
    "    count = 0.0\n",
    "    \n",
    "    for (abstract_curr, abstract_vector_curr, year_curr) in zip(abstracts_curr, abstract_vectors_curr, years_curr):\n",
    "        min_distance = np.inf\n",
    "        abstract_min, abstract_vector_min, year_min = None, None, None\n",
    "        abstract_rmin, abstract_vector_rmin, year_rmin = None, None, None\n",
    "        for (abstract_prev, abstract_vector_prev, year_prev) in zip(abstracts_prev, abstract_vectors_prev, years_prev):\n",
    "            if dist(abstract_vector_curr, abstract_vector_prev) < min_distance:\n",
    "                min_distance = dist(abstract_vector_curr, abstract_vector_prev)\n",
    "                abstract_min, abstract_vector_min, year_min = abstract_curr, abstract_vector_curr, year_curr\n",
    "                abstract_rmin, abstract_vector_rmin, year_rmin = abstract_prev, abstract_vector_prev, year_prev\n",
    "                \n",
    "        ds.append(min_distance)\n",
    "        abstracts_r.append(abstract_rmin)\n",
    "        years_r.append(year_rmin)\n",
    "        \n",
    "        if min_distance > CUTOFF_DISTANCE:\n",
    "            count += 1\n",
    "                \n",
    "    ds_sorted, abstracts_r_sorted, years_r_sorted, abstracts_curr_sorted = zip(*sorted(zip(ds, abstracts_r, years_r, abstracts_curr), key=lambda x: -x[0]))\n",
    "                                    \n",
    "    abstracts_prev = np.concatenate((abstracts_prev, abstracts_curr))\n",
    "    abstract_vectors_prev = np.concatenate((abstract_vectors_prev, abstract_vectors_curr))\n",
    "    years_prev = np.concatenate((years_prev, years_curr))\n",
    "    \n",
    "    counts.append(count / len(years_curr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(year_span[1:], counts, color='b', s=100)\n",
    "plt.plot(year_span[1:], counts, color='b')\n",
    "plt.title(f'Semantic Distance Novelty for {term} from Abstracts')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of Novel Papers')\n",
    "plt.rc('font', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify clusters of novel research based on paper abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify research papers published between year CHANGEPOINT and CHANGEPOINT + DELTA,  such that\n",
    "# (the mean distance between the paper abstract and its K nearest neighbors published before week CHANGEPOINT)\n",
    "# is at least RATIO_THRESHOLD times (the mean distance between the paper abstract and its K nearest neighbors\n",
    "# also published between CHANGEPOINT and CHANGEPOINT + DELTA)\n",
    "CHANGEPOINT = 2000\n",
    "K = 3\n",
    "DELTA = 5\n",
    "RATIO_THRESHOLD = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_old, abstract_vectors_old, years_old = [], [], []\n",
    "abstracts_new, abstract_vectors_new, years_new, titles_new = [], [], [], []\n",
    "\n",
    "for abstract, abstract_vector, year in zip(abstracts, abstract_vectors, years):\n",
    "    if year < CHANGEPOINT:\n",
    "        abstracts_old.append(abstract)\n",
    "        abstract_vectors_old.append(abstract_vector)\n",
    "        years_old.append(year)\n",
    "    if year >= CHANGEPOINT and year <= CHANGEPOINT + DELTA:\n",
    "        abstracts_new.append(abstract)\n",
    "        abstract_vectors_new.append(abstract_vector)\n",
    "        years_new.append(year)\n",
    "        titles_new.append(title)\n",
    "            \n",
    "for abstract, abstract_vector, year, title in zip(abstracts_new, abstract_vectors_new, years_new, titles_new):\n",
    "    new_distance = np.mean(sorted([dist(abstract_vector, t_v) for t_v in abstract_vectors_new])[1:][:K])\n",
    "    old_distance = np.mean(sorted([dist(abstract_vector, t_v) for t_v in abstract_vectors_old])[:K])\n",
    "    \n",
    "    if (old_distance / new_distance) > RATIO_THRESHOLD:\n",
    "        print(f'Year: {year}')\n",
    "        print(f'Ratio: {old_distance / new_distance}')\n",
    "        print(f'New Distance: {new_distance}')\n",
    "        print(f'Old Distance: {old_distance}')\n",
    "        print(f'Title: {title}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create differential wordclouds for paper abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a differential wordcloud for words appearing in year2, vs all words appearing before or during year1\n",
    "def create_wordcloud(year1, year2):\n",
    "    freqs_prev = np.sum(v[years <= year1], axis=0) / np.sum(years <= year1)\n",
    "    freqs_new = np.sum(v[years == year2], axis=0) / np.sum(years == year2)\n",
    "\n",
    "    diffs = np.subtract(freqs_prev, freqs_new)\n",
    "\n",
    "    pos_features, neg_features = set(), set()\n",
    "\n",
    "    d = {}\n",
    "    for diff, feature in zip(diffs, features_unsorted):\n",
    "        if feature in stop_words:\n",
    "            continue\n",
    "\n",
    "        diff = round(diff * 1000)\n",
    "        if diff > 0:\n",
    "            d[feature] = diff\n",
    "            pos_features.add(feature)\n",
    "        if diff < 0:\n",
    "            neg_features.add(feature) \n",
    "            d[feature] = -diff\n",
    "\n",
    "    def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        if word in pos_features:\n",
    "            return \"rgb(0, 0, 0)\"\n",
    "        if word in neg_features:\n",
    "            return \"rgb(256, 0, 0)\"\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white', height=500, width=1000)\n",
    "    wordcloud.generate_from_frequencies(frequencies=d)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud.recolor(color_func=color_func, random_state=3),\n",
    "               interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_wordcloud(1999, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
